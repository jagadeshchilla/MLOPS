# ğŸš€ Apache Airflow with Astro CLI - Complete Setup Guide

## ğŸ“š Table of Contents

- [Introduction to Apache Airflow](#-introduction-to-apache-airflow)
- [Key Components of Apache Airflow](#-key-components-of-apache-airflow)
- [Why Airflow for MLOps](#-why-airflow-for-mlops)
- [Setting Up Airflow with Astro](#-setting-up-airflow-with-astro)
- [Project Overview](#-project-overview)
- [What We've Accomplished](#-what-weve-accomplished)
- [Getting Started](#-getting-started)
- [DAGs in This Project](#-dags-in-this-project)
- [Troubleshooting](#-troubleshooting)

---

## ğŸŒŸ Introduction to Apache Airflow

Apache Airflow is an **open-source platform** designed to programmatically author, schedule, and monitor workflows. It allows you to define workflows as **Directed Acyclic Graphs (DAGs)** of tasks, making complex data pipelines manageable and observable.

### Key Features:
- **Dynamic**: Pipelines are configuration as code (Python), allowing for dynamic pipeline generation
- **Extensible**: Easily define your own operators, executors and extend the library
- **Elegant**: Airflow pipelines are lean and explicit with parameterization built into the core
- **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate workers

---

## ğŸ”§ Key Components of Apache Airflow

### 1. **Web Server** ğŸŒ
- Provides the user interface for monitoring and managing workflows
- Accessible at `http://localhost:8080` (in our setup)
- Offers DAG visualization, task monitoring, and log inspection

### 2. **Scheduler** â°
- Heart of Airflow that triggers scheduled workflows
- Monitors all tasks and DAGs, triggering task instances when dependencies are met
- Handles task scheduling based on defined intervals

### 3. **Executor** âš¡
- Defines how and where tasks are executed
- Can run tasks locally, on multiple machines, or in containers
- Our setup uses the default LocalExecutor

### 4. **Metadata Database** ğŸ—„ï¸
- Stores all metadata about DAGs, tasks, variables, and connections
- PostgreSQL database running on port `5433` in our configuration
- Critical for maintaining state and history

### 5. **Workers** ğŸ‘·
- Execute the actual tasks defined in your DAGs
- Can be scaled horizontally for increased throughput

---

## ğŸ¤– Why Airflow for MLOps

Machine Learning Operations (MLOps) requires robust orchestration of complex workflows. Airflow excels in this domain because:

### **Data Pipeline Management** ğŸ“Š
- **ETL/ELT Processes**: Extract, transform, and load data from various sources
- **Data Validation**: Ensure data quality before model training
- **Feature Engineering**: Automate feature creation and selection

### **Model Lifecycle Orchestration** ğŸ”„
- **Training Pipelines**: Automate model training with different parameters
- **Model Validation**: Automated testing and validation of model performance
- **Deployment Automation**: Seamless model deployment to production environments

### **Monitoring & Observability** ğŸ‘ï¸
- **Pipeline Monitoring**: Track data pipeline health and performance
- **Model Drift Detection**: Monitor model performance over time
- **Alerting**: Automated notifications for pipeline failures or model degradation

### **Scalability & Reliability** ğŸ—ï¸
- **Retry Logic**: Automatic retry of failed tasks with configurable policies
- **Dependency Management**: Complex task dependencies handled elegantly
- **Resource Management**: Efficient resource allocation and scaling

---

## ğŸ› ï¸ Setting Up Airflow with Astro

**Astro CLI** is the fastest way to get started with Apache Airflow. It provides:

- **Pre-configured Environment**: Ready-to-use Airflow setup with Docker
- **Local Development**: Test DAGs locally before deployment
- **Production Ready**: Easy deployment to Astronomer's managed service
- **Best Practices**: Built-in best practices and optimizations

### Why Choose Astro CLI?
- âœ… **Zero Configuration**: No manual setup of databases, web servers, or schedulers
- âœ… **Docker-based**: Consistent environment across development and production
- âœ… **Hot Reloading**: Changes to DAGs are reflected immediately
- âœ… **Integrated Tooling**: Built-in testing, linting, and deployment tools

---

## ğŸ“‹ Project Overview

This project demonstrates a complete Apache Airflow setup using Astro CLI, featuring multiple DAGs that showcase different aspects of workflow orchestration.

### **Project Structure**
```
airflow_ASTRO/
â”œâ”€â”€ dags/                          # DAG definitions
â”‚   â”œâ”€â”€ mlpipeline.py             # Machine Learning pipeline
â”‚   â”œâ”€â”€ maths_operation.py        # Mathematical operations chain
â”‚   â””â”€â”€ exampledag.py             # Example DAG from template
â”œâ”€â”€ plugins/                       # Custom plugins
â”œâ”€â”€ include/                       # Additional files
â”œâ”€â”€ tests/                         # DAG tests
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ Dockerfile                     # Container configuration
â”œâ”€â”€ .env                          # Environment variables
â””â”€â”€ README.md                     # This file
```

---

## âœ… What We've Accomplished

### **1. Environment Setup** ğŸ—ï¸
- âœ… **Installed Astro CLI** (v1.34.1) using Windows Package Manager
- âœ… **Resolved Port Conflicts** by configuring custom ports:
  - PostgreSQL: `5433` (avoiding conflict with existing service on `5432`)
  - Airflow Web Server: `8080`
- âœ… **Successfully Started** all Airflow components in Docker containers

### **2. DAG Development** ğŸ“
- âœ… **Created ML Pipeline DAG** (`mlpipeline.py`):
  - Data preprocessing task
  - Model training task  
  - Model evaluation task
  - Sequential task dependencies
  
- âœ… **Built Mathematical Operations DAG** (`maths_operation.py`):
  - Chain of mathematical operations using XCom
  - Demonstrates data passing between tasks
  - Shows complex workflow orchestration

### **3. Compatibility Fixes** ğŸ”§
- âœ… **Fixed Import Issues**: Updated deprecated `python_operator` imports
- âœ… **Resolved Schedule Parameter**: Changed `schedule_interval` to `schedule`
- âœ… **Removed Deprecated Parameters**: Eliminated `provide_context=True` usage
- âœ… **Ensured Modern Airflow Compatibility**: All DAGs work with Airflow 2.8+

### **4. Configuration Optimization** âš™ï¸
- âœ… **Custom Port Configuration**: Avoided system conflicts
- âœ… **Environment Variables**: Proper `.env` file setup
- âœ… **Docker Integration**: Seamless container orchestration

---

## ğŸš€ Getting Started

### **Prerequisites**
- Windows 10/11 with WSL2 enabled
- Windows Package Manager (`winget`)
- Docker or Podman installed

### **Quick Start**
1. **Clone this repository**
   ```bash
   git clone <repository-url>
   cd airflow_ASTRO
   ```

2. **Start the Airflow environment**
   ```bash
   astro dev start
   ```

3. **Access the Airflow UI**
   - Open your browser to `http://localhost:8080`
   - Login with credentials: `admin` / `admin`

4. **Explore the DAGs**
   - Navigate to the main DAGs view
   - Enable and trigger the example DAGs

---

## ğŸ“Š DAGs in This Project

### **1. ML Pipeline (`mlpipeline.py`)** ğŸ¤–
```python
# Workflow: Data â†’ Model â†’ Evaluation
preprocess_data â†’ train_model â†’ evaluate_model
```
- **Purpose**: Demonstrates a typical ML workflow
- **Schedule**: Weekly (`@weekly`)
- **Tasks**: 3 sequential tasks representing ML pipeline stages

### **2. Mathematical Operations (`maths_operation.py`)** ğŸ”¢
```python
# Workflow: 10 â†’ +5 â†’ Ã—2 â†’ -3 â†’ ^2 = 529
start(10) â†’ add_5(15) â†’ multiply_by_2(30) â†’ subtract_3(27) â†’ square(729)
```
- **Purpose**: Shows XCom data passing between tasks
- **Schedule**: Run once (`@once`)
- **Tasks**: 5 chained mathematical operations

### **3. Example DAG (`exampledag.py`)** ğŸ“
- **Purpose**: Template-generated example DAG
- **Features**: Demonstrates various Airflow concepts

---

## ğŸ” Troubleshooting

### **Common Issues & Solutions**

#### **Port Conflicts**
```bash
# Error: Port 5432 already in use
# Solution: We've configured custom ports
POSTGRES_PORT=5433
AIRFLOW_WEBSERVER_PORT=8080
```

#### **Import Errors**
```python
# Old (deprecated):
from airflow.operators.python_operator import PythonOperator

# New (correct):
from airflow.operators.python import PythonOperator
```

#### **Schedule Parameter Issues**
```python
# Old (deprecated):
schedule_interval='@weekly'

# New (correct):
schedule='@weekly'
```

#### **Context Issues**
```python
# Old (deprecated):
PythonOperator(
    task_id='my_task',
    python_callable=my_function,
    provide_context=True  # Remove this line
)

# New (automatic):
PythonOperator(
    task_id='my_task',
    python_callable=my_function  # Context provided automatically
)
```
